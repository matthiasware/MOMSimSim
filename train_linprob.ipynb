{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# non torch\n",
    "from dotted_dict import DottedDict\n",
    "import pprint\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "\n",
    "# local\n",
    "from utils import AverageMeter, get_dataset, get_backbone, get_optimizer, get_scheduler\n",
    "from augmentations import get_aug\n",
    "from model import SimSiam, DownStreamClassifier\n",
    "import utils\n",
    "import configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_ckpt = Path(\n",
    "    \"/mnt/experiments/simsiam/run_cifar10_resnet18_20201204-135350/ckpts/model_cifar10_epoch_000099.ckpt\")\n",
    "assert p_ckpt.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(p_ckpt)\n",
    "\n",
    "train_config = ckpt[\"config\"]\n",
    "pp.pprint(train_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configs.get_config(train_config.dataset,train=False)\n",
    "\n",
    "pp.pprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "train_set = get_dataset(\n",
    "    train_config.dataset,\n",
    "    train_config.p_data,\n",
    "    transform=get_aug(train_config.img_size, train=True, train_classifier=True, means_std=train_config.mean_std),\n",
    "    train=True,\n",
    "    download=False\n",
    ")\n",
    "if train_config.dataset == \"stl10\":\n",
    "    # stl10 has only 5000 labeled samples in its train set\n",
    "    train_set = torch.utils.data.Subset(train_set, range(0, 5000))\n",
    "\n",
    "test_set = get_dataset(\n",
    "    train_config.dataset,\n",
    "    train_config.p_data,\n",
    "    transform=get_aug(train_config.img_size, train=True, train_classifier=True, means_std=train_config.mean_std),\n",
    "    train=False,\n",
    "    download=False\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=config.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_set,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=config.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_loader))\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "backbone = get_backbone(train_config.backbone)\n",
    "model = SimSiam(backbone, train_config.projector_args, train_config.predictor_args)\n",
    "\n",
    "# load weights\n",
    "#msg = model.load_state_dict(ckpt[\"state_dict\"], strict=True)\n",
    "#print(\"Loading weights: {}\".format(msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DownStreamClassifier(model, 2048, 512, 10).to(config.device)\n",
    "#for name, param in model.named_parameters():\n",
    "#    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(config.optimizer, model, config.optimizer_args)\n",
    "lr_scheduler = lr_scheduler = get_scheduler(config.scheduler, optimizer, config.scheduler_args)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.5, weight_decay=5e-4, momentum=0.9)\n",
    "#lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=0.0001)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train(epoch, train_loader, model, optimizer, criterion):\n",
    "    model.train()\n",
    "\n",
    "    losses, acc, step, total = 0., 0., 0., 0.\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(config.device), target.to(config.device)\n",
    "\n",
    "        logits = model(data)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(logits, target)\n",
    "        loss.backward()\n",
    "        losses += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "        pred = F.softmax(logits, dim=-1).max(-1)[1]\n",
    "        acc += pred.eq(target).sum().item()\n",
    "\n",
    "        step += 1\n",
    "        total += target.size(0)\n",
    "\n",
    "    print('[Train Epoch: {0:4d}], loss: {1:.3f}, acc: {2:.3f}'.format(epoch, losses / step, acc / total * 100.))\n",
    "\n",
    "\n",
    "def _eval(epoch, test_loader, model, criterion):\n",
    "    model.eval()\n",
    "\n",
    "    losses, acc, step, total = 0., 0., 0., 0.\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(config.device), target.to(config.device)\n",
    "\n",
    "            logits = model(data)\n",
    "            loss = criterion(logits, target)\n",
    "            losses += loss.item()\n",
    "            pred = F.softmax(logits, dim=-1).max(-1)[1]\n",
    "            acc += pred.eq(target).sum().item()\n",
    "\n",
    "            step += 1\n",
    "            total += target.size(0)\n",
    "        print('[Test Epoch: {0:4d}], loss: {1:.3f}, acc: {2:.3f}'.format(epoch, losses / step, acc / total * 100.))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 100):\n",
    "    _train(epoch, train_loader, model, optimizer, criterion)\n",
    "    _eval(epoch, test_loader, model, criterion)\n",
    "    lr_scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
