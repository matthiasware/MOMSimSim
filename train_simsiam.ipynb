{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "import pprint\n",
    "#\n",
    "import configs\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.models import resnet50\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from model import SimSiam\n",
    "from utils import AverageMeter, get_dataset, get_backbone, get_optimizer, get_scheduler\n",
    "from augmentations import get_aug\n",
    "from dotted_dict import DottedDict\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "DEBUG = True\n",
    "DEVICE = \"cuda:1\"\n",
    "DATASET = \"cifar10\"\n",
    "\n",
    "config = configs.get_config(dataset=DATASET,\n",
    "                            train=True,\n",
    "                            debug=DEBUG,\n",
    "                            device=DEVICE)\n",
    "\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "pp.pprint(config)\n",
    "\n",
    "# prepare data\n",
    "train_transform = get_aug(img_size=config.img_size,\n",
    "                          train=True,\n",
    "                          train_classifier=False,\n",
    "                          means_std=config.mean_std)\n",
    "\n",
    "classifier_transform = get_aug(img_size=config.img_size,\n",
    "                               train=False,\n",
    "                               train_classifier=True,\n",
    "                               means_std=config.mean_std)\n",
    "#\n",
    "train_set = get_dataset(config.dataset, config.p_data,\n",
    "                        train=True, transform=train_transform)\n",
    "classifier_set = get_dataset(config.dataset, config.p_data,\n",
    "                             train=False, transform=classifier_transform)\n",
    "#\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=config.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "classifier_loader = torch.utils.data.DataLoader(\n",
    "    dataset=classifier_set,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=config.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# model\n",
    "backbone = get_backbone(config.backbone)\n",
    "model = SimSiam(backbone, config.projector_args, config.predictor_args)\n",
    "model = model.to(config.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(config.optimizer, model, config.optimizer_args)\n",
    "\n",
    "# define lr scheduler\n",
    "lr_scheduler = get_scheduler(\n",
    "    config.scheduler, optimizer, config.scheduler_args)\n",
    "\n",
    "loss_meter = AverageMeter(\"loss\")\n",
    "\n",
    "writer = SummaryWriter(config.p_logs)\n",
    "\n",
    "# create train dir\n",
    "config.p_logs.mkdir(exist_ok=True, parents=True)\n",
    "config.p_ckpts.mkdir(exist_ok=True, parents=True)\n",
    "#\n",
    "# tensorboard writer\n",
    "writer = SummaryWriter(config.p_logs)\n",
    "print(\"tensorboard --logdir={} --host=0.0.0.0\".format(str(config.p_logs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, config.num_epochs + 1):\n",
    "    #\n",
    "    # TRAIN LOOP\n",
    "    #\n",
    "    loss_meter.reset()\n",
    "    model.train()\n",
    "    p_bar = tqdm(train_loader, desc=f'Epoch {epoch}/{config.num_epochs}')\n",
    "    for idx, ((images1, images2), labels) in enumerate(p_bar):\n",
    "        model.zero_grad()\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = model.backbone(images1.to(config.device))\n",
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.projector(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, config.num_epochs + 1):\n",
    "    #\n",
    "    # TRAIN LOOP\n",
    "    #\n",
    "    loss_meter.reset()\n",
    "    model.train()\n",
    "    p_bar = tqdm(train_loader, desc=f'Epoch {epoch}/{config.num_epochs}')\n",
    "    for idx, ((images1, images2), labels) in enumerate(p_bar):\n",
    "        model.zero_grad()\n",
    "        loss = model.forward(images1.to(config.device),\n",
    "                             images2.to(config.device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_meter.update(loss.item())\n",
    "        #\n",
    "        writer.add_scalar('loss', loss_meter.val,\n",
    "                          epoch * len(train_loader) + idx)\n",
    "        writer.add_scalar('avg_loss', loss_meter.avg,\n",
    "                          epoch * len(train_loader) + idx)\n",
    "        p_bar.set_postfix({\"loss\": loss_meter.val, 'loss_avg': loss_meter.avg, 'lr': lr_scheduler.get_last_lr()[0]})\n",
    "\n",
    "    writer.add_scalar('epoch_avg', loss_meter.avg, epoch)\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    # Save checkpoint\n",
    "    p_ckpt = config.p_ckpts / config.fs_ckpt.format(config.dataset, epoch)\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        # 'optimizer':optimizer.state_dict(), # will double the checkpoint file size\n",
    "        'lr_scheduler': lr_scheduler.state_dict(),\n",
    "        'config': config,\n",
    "        'loss_meter': loss_meter\n",
    "    }, p_ckpt)\n",
    "    print(f\"Model saved to {p_ckpt}\")\n",
    "    #\n",
    "    # CLASSIFIER LOOP\n",
    "    #\n",
    "    if epoch % config.freq_knn == 0:\n",
    "        model.eval()\n",
    "        all_projections = []\n",
    "        all_labels = []\n",
    "        for images, labels in classifier_loader:\n",
    "            with torch.no_grad():\n",
    "                z = model.encoder(images.to(config.device))\n",
    "                all_projections.append(z.cpu().numpy())\n",
    "                all_labels.append(labels.cpu().numpy())\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "        all_projections = np.concatenate(all_projections)\n",
    "        \n",
    "        # knn \n",
    "        neigh = KNeighborsClassifier(n_neighbors=5, algorithm='brute', n_jobs=8)\n",
    "        neigh.fit(all_projections, all_labels)\n",
    "        score = neigh.score(all_projections, all_labels)\n",
    "        writer.add_scalar('knn_acc', score, epoch)\n",
    "\n",
    "        # std\n",
    "        # ideally around 1/np.sqrt(d)\n",
    "        norms = np.linalg.norm(all_projections, axis=1)\n",
    "        z_bars = all_projections / norms[:, None]\n",
    "        std = z_bars.std(axis=0).mean()\n",
    "\n",
    "        writer.add_scalar('std', std, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "dataset = \"cifar10\"\n",
    "\n",
    "config = configs.get_config(dataset)\n",
    "\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "pp.pprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.device = \"cuda:1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "train_transform = get_aug(img_size=config.img_size,\n",
    "                          train=True,\n",
    "                          train_classifier=False,\n",
    "                          means_std=config.mean_std)\n",
    "\n",
    "classifier_transform = get_aug(img_size=config.img_size,\n",
    "                               train=False,\n",
    "                               train_classifier=True,\n",
    "                               means_std=config.mean_std)\n",
    "#\n",
    "train_set = get_dataset(config.dataset, config.p_data,\n",
    "                        train=True, transform=train_transform)\n",
    "classifier_set = get_dataset(config.dataset, config.p_data,\n",
    "                             train=False, transform=classifier_transform)\n",
    "if config.debug:\n",
    "    # take only one batch\n",
    "    train_set = torch.utils.data.Subset(train_set, range(0, config.batch_size))\n",
    "#\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=config.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "classifier_loader = torch.utils.data.DataLoader(\n",
    "    dataset=classifier_set,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=config.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# model\n",
    "backbone = get_backbone(config.backbone)\n",
    "model = SimSiam(backbone, config.projector_args, config.predictor_args)\n",
    "model = model.to(config.device)\n",
    "\n",
    "optimizer = get_optimizer(config.optimizer, model, config.optimizer_args)\n",
    "\n",
    "# define lr scheduler\n",
    "lr_scheduler = get_scheduler(\n",
    "    config.scheduler, optimizer, config.scheduler_args)\n",
    "\n",
    "loss_meter = AverageMeter(\"loss\")\n",
    "\n",
    "writer = SummaryWriter(config.p_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train dir\n",
    "config.p_logs.mkdir(exist_ok=True, parents=True)\n",
    "config.p_ckpts.mkdir(exist_ok=True, parents=True)\n",
    "#\n",
    "# tensorboard writer\n",
    "writer = SummaryWriter(config.p_logs)\n",
    "print(\"tensorboard --logdir={} --host=0.0.0.0\".format(str(config.p_logs)))\n",
    "#\n",
    "for epoch in range(1, config.num_epochs + 1):\n",
    "    #\n",
    "    # CLASSIFIER LOOP\n",
    "    #\n",
    "    if epoch % config.freq_knn == 0:\n",
    "        model.eval()\n",
    "        all_projections = []\n",
    "        all_labels = []\n",
    "        for images, labels in classifier_loader:\n",
    "            with torch.no_grad():\n",
    "                z = model.encoder(images.to(config.device))\n",
    "                all_projections.append(z.cpu().numpy())\n",
    "                all_labels.append(labels.cpu().numpy())\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "        all_projections = np.concatenate(all_projections)\n",
    "        #\n",
    "        neigh = KNeighborsClassifier(n_neighbors=5, algorithm='brute', n_jobs=8)\n",
    "        neigh.fit(all_projections, all_labels)\n",
    "        score = neigh.score(all_projections, all_labels)\n",
    "        writer.add_scalar('knn_acc', score, epoch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_projections.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = np.linalg.norm(all_projections, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(all_projections[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_bars = all_projections / norms[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_projections[0] / np.linalg.norm(all_projections[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_bars.std(axis=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
